{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Starting With Data\"\n",
    "teaching: 25\n",
    "exercises: 15\n",
    "questions:\n",
    "- \"How does Python deal with data tables?\"\n",
    "objectives:\n",
    "- \"Explain what a library is, and what libraries are used for.\"\n",
    "- \"Load a Python/Pandas library.\"\n",
    "- \"Read tabular data from a file into Python using Pandas using *read_csv*.\"\n",
    "- \"Learn about the Pandas DataFrame object.\"\n",
    "- \"Learn about data slicing and indexing.\"\n",
    "- \"Perform mathematical operations on numeric data.\"\n",
    "- \"Create simple plots of data.\"\n",
    "\n",
    "keypoints:\n",
    "- \"Core concepts in python: Python libraries, Pandas DataFrames, working with data.\"\n",
    "---\n",
    "\n",
    "\n",
    "# Working with Pandas DataFrames in Python\n",
    "\n",
    "## Working with Library Head Count Data\n",
    "\n",
    "For this lesso, we will use head count data provided by USC's Leavey library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentation of the DOAJ Articles data\n",
    "\n",
    "For this lesson, we will be using Directory of Open Access Journals (DOAJ) article sample data, available on [FigShare](https://dx.doi.org/10.6084/m9.figshare.3409471). Download this zip\n",
    "and extract it on your working directory on a meaningful location (e.g. create\n",
    "a folder called *data/*)\n",
    "\n",
    "This data set is a list of published articles. The dataset is stored as *.csv*\n",
    "(comma separated values) files: each row holds information for a single article,\n",
    "and the columns represent:\n",
    "\n",
    "| Column           | Description                        |\n",
    "|------------------|------------------------------------|\n",
    "| Title            | Title of the article               |\n",
    "| Authors          | Author (or authors)                |\n",
    "| DOI              | DOI                                |\n",
    "| URL              | URL                                |\n",
    "| Subjects         | List of subject key words          |\n",
    "| ISSNs            | ISSNs code                         |\n",
    "| Citation         | Citation information               |\n",
    "| LanguageId       | Language identifier                |\n",
    "| LicenceId        | License identifier                 |\n",
    "| Author_Count     | Number of authors of the article   |\n",
    "| First_Author     | Name of the first author           |\n",
    "| Citation_Count   | Number times it has been cited     |\n",
    "| Day              | Day of publication                 |\n",
    "| Month            | Month of publication               |\n",
    "| Year             | Year of publication                |\n",
    "\n",
    "## About (Software) Libraries\n",
    "\n",
    "A library in Python contains a set of tools (called functions) that perform\n",
    "tasks on our data. Importing a library is like getting a piece of lab equipment\n",
    "out of a storage locker and setting it up on the bench for use in a project.\n",
    "Once a library is set up, it can be used or called to perform many tasks.\n",
    "\n",
    "## Pandas in Python\n",
    "\n",
    "One of the best options for working with tabular data in Python is to use the [pandas](http://pandas.pydata.org/) data analysis library. \n",
    "Pandas provides data structures, produces high quality plots with [matplotlib](http://matplotlib.org/), and integrates nicely with other libraries that use [NumPy](http://www.numpy.org/) (which is another Python library) arrays.\n",
    "\n",
    "Python doesn't load all of the libraries available to it by default. We have to\n",
    "add an `import` statement to our code in order to use library functions. To import\n",
    "a library, we use the syntax `import libraryName`. If we want to give the\n",
    "library a nickname to shorten the command, we can add `as nickNameHere`.  An\n",
    "example of importing the pandas library using the common nickname `pd` is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .source}\n",
    "\n",
    "Each time we call a function that's in a library, we use the syntax\n",
    "`LibraryName.FunctionName`. Adding the library name with a `.` before the\n",
    "function name tells Python where to find the function. In the example above, we\n",
    "have imported Pandas as `pd`. This means we don't have to type out `pandas` each\n",
    "time we call a Pandas function.\n",
    "\n",
    "## Lesson Overview\n",
    "\n",
    "For this lesson we will be using the Directory of Open Access Journals (DOAJ) article data.\n",
    "\n",
    "We are analyzing the articles published in a particular field of study. The\n",
    "data set is stored in *.csv* (comma separated values) format. Within\n",
    "the *.csv* files, each row holds information for a single article.\n",
    "\n",
    "The first few rows of our first file (articles.csv) look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id,Title,Authors,DOI,URL,Subjects,ISSNs,Citation,LanguageId,LicenceId,Author_Count,First_Author,Citation_Count,Day,Month,Year\n",
    "0,The Fisher Thermodynamics of Quasi-Probabilities,Flavia Pennini|Angelo Plastino,10.3390/e17127853,https://doaj.org/article/b75e8d5cca3f46cbbd63e91be5b32412,Fisher information|quasi-probabilities|complementarity|Physics|QC1-999|Science|Q,1099-4300,\"Entropy, Vol 17, Iss 12, Pp 7848-7858 (2015)\",1,1,2,Flavia Pennini,4,1,11,2015\n",
    "1,Aflatoxin Contamination of the Milk Supply: A Pakistan Perspective,Naveed Aslam|Peter C. Wynn,10.3390/agriculture5041172,https://doaj.org/article/0edc5af6672641c0bd45608812a34f9e,aflatoxins|AFM1|AFB1|milk marketing chains|hepatocellular carcinoma|Agriculture (General)|S1-972|Agriculture|S,2077-0472,\"Agriculture (Basel), Vol 5, Iss 4, Pp 1172-1182 (2015)\",1,1,2,Naveed Aslam,5,1,11,2015\n",
    "2,Metagenomic Analysis of Upwelling-Affected Brazilian Coastal Seawater Reveals Sequence Domains of Type I PKS and Modular NRPS,Rafael R. C. Cuadrat|Juliano C. Cury|Alberto M. R. Dávila,10.3390/ijms161226101,https://doaj.org/article/d9fe469f75a0442382b84ba4f50007ee,PKS|NRPS|metagenomics|environmental genomics|upwelling|coastal environment|Chemistry|QD1-999|Science|Q,1422-0067,\"International Journal of Molecular Sciences, Vol 16, Iss 12, Pp 28285-28295 (2015)\",1,1,3,Rafael R. C. Cuadrat,8,1,11,2015\n",
    "3,\"Synthesis and Reactivity of a Cerium(III) Scorpionate Complex Containing a Redox Non-Innocent 2,2′-Bipyridine Ligand\",Fabrizio Ortu|Hao Zhu|Marie-Emmanuelle Boulon|David P. Mills,10.3390/inorganics3040534,https://doaj.org/article/95606ed39deb4f43b96f7e6308ad15d3,lanthanide|cerium|scorpionate|tris(pyrazolyl)borate|radical|redox non-innocent|Inorganic chemistry|QD146-197,2304-6740,\"Inorganics (Basel), Vol 3, Iss 4, Pp 534-553 (2015)\",1,1,4,Fabrizio Ortu,5,1,11,2015\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .output}\n",
    "(quite difficult to read and interpret as it is...)\n",
    "\n",
    "### We want to:\n",
    "\n",
    "1. Load that data into memory using Python.\n",
    "2. Calculate the average number of authors per article, for each publisher.\n",
    "3. Plot this information.\n",
    "\n",
    "We can automate the process above using Python. It is efficient to spend time\n",
    "building the code to perform these tasks because once it is built, we can use it\n",
    "over and over on different datasets that use a similar format. This makes our\n",
    "methods easily reproducible. We can also easily share our code with colleagues\n",
    "and they can replicate the same analysis.\n",
    "\n",
    "# Reading CSV data using Pandas\n",
    "\n",
    "We will begin by locating and reading our survey data which are in CSV format (comma separated values).\n",
    "We can use Pandas' `read_csv` function to pull the file directly into a\n",
    "[DataFrame](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe).\n",
    "\n",
    "## So what's a DataFrame?\n",
    "\n",
    "A DataFrame is a 2-dimensional data structure that can store data of different\n",
    "types (including characters, integers, floating point values, factors and more)\n",
    "in columns. It is similar to a spreadsheet or an SQL table or the *data.frame* in\n",
    "R. A DataFrame always has an index (0-based). An index refers to the position of\n",
    "an element in the data structure.\n",
    "\n",
    "First, let's make sure the Python Pandas library is loaded. We will import\n",
    "Pandas using the nickname `pd`.  This is a common convention on the internet,\n",
    "so if you look up Pandas usage, you will often see it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .source}\n",
    "\n",
    "Let's also import the [OS Library](https://docs.python.org/3/library/os.html).\n",
    "This library allows us to make sure we are in the correct working directory. If\n",
    "you are working in IPython or Jupyter Notebook, be sure to start the notebook in the\n",
    "workshop repository.  If you didn't do that you can always set the working\n",
    "directory using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "# if this directory isn't right, use the command below to set the working directory\n",
    "os.chdir(\"YOURPathHere\")\n",
    "\n",
    "# note that pd.read_csv is used because we imported pandas as pd\n",
    "pd.read_csv(\"articles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .source}\n",
    "\n",
    "The above command yields the **output** below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        id                                              Title  \\\n",
    "0        0   The Fisher Thermodynamics of Quasi-Probabilities   \n",
    "1        1  Aflatoxin Contamination of the Milk Supply: A ...   \n",
    "2        2  Metagenomic Analysis of Upwelling-Affected Bra...   \n",
    "3        3  Synthesis and Reactivity of a Cerium(III) Scor...   \n",
    "...\n",
    "997    997  Crystal structure of bis(3-bromopyridine-κN)bi...   \n",
    "998    998  Crystal structure of 4,4′-(ethane-1,2-diyl)bis...   \n",
    "999    999  Crystal structure of (Z)-4-[1-(4-acetylanilino...   \n",
    "1000  1000  Metagenomic Analysis of Upwelling-Affected Bra...\n",
    "\n",
    "...\n",
    "\n",
    "      Month  Year  \n",
    "0        11  2015  \n",
    "1        11  2015  \n",
    "2        11  2015  \n",
    "...\n",
    "998       1  2015  \n",
    "999       1  2015  \n",
    "1000     11  2015  \n",
    "\n",
    "[1001 rows x 16 columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .output}\n",
    "\n",
    "We can see that there were 1,001 rows parsed. Each row has 11\n",
    "columns. The first column is the index of the DataFrame. The index is used to\n",
    "identify the position of the data, but it is not an actual column of the DataFrame.\n",
    "It looks like  the `read_csv` function in Pandas  read our file properly. However,\n",
    "we haven't saved any data to memory so we can work with it. We need to assign the\n",
    "DataFrame to a variable. Remember that a variable is a name for a value, such as `x`,\n",
    "or  `data`. We can create a new  object with a variable name by assigning a value to it using `=`.\n",
    "\n",
    "Let's call the imported survey data `articles_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.read_csv(\"articles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .source}\n",
    "\n",
    "Notice when you assign the imported DataFrame to a variable, Python does not\n",
    "produce any output on the screen. We can print the value of the `articles_df`\n",
    "object by typing its name into the Python command prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .source}\n",
    "\n",
    "which prints contents like above.\n",
    "\n",
    "## Manipulating our Articles data\n",
    "\n",
    "Now we can start manipulating our data. First, let's check the data type of the\n",
    "data stored in `articles_df` using the `type` method. The `type` method and\n",
    "`__class__` attribute tell us that `articles_df` type is `<class 'pandas.core.frame.DataFrame'>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(articles_df)\n",
    "# this does the same thing as the above!\n",
    "articles_df.__class__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .source}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<class 'pandas.core.frame.DataFrame'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .output}\n",
    "\n",
    "We can also enter `articles_df.dtypes` at our prompt to view the data type for each\n",
    "column in our DataFrame.\n",
    "* `int64` represents numeric integer values (`int64` cells\n",
    "can not store decimals).\n",
    "* `object` represents strings (letters and numbers).\n",
    "* `float64`\n",
    "represents numbers with decimals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .source}\n",
    "\n",
    "which returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id                 int64\n",
    "Title             object\n",
    "Authors           object\n",
    "DOI               object\n",
    "URL               object\n",
    "Subjects          object\n",
    "ISSNs             object\n",
    "Citation          object\n",
    "LanguageId         int64\n",
    "LicenceId          int64\n",
    "Author_Count       int64\n",
    "First_Author      object\n",
    "Citation_Count     int64\n",
    "Day                int64\n",
    "Month              int64\n",
    "Year               int64\n",
    "dtype: object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .output}\n",
    "\n",
    "We'll talk a bit more about what the different data types mean later in [Data Types and Formats](/03-data-types-and-format/).\n",
    "\n",
    "### Useful ways to view DataFrame objects in Python\n",
    "\n",
    "There are multiple methods that can be used to summarize and access the data\n",
    "stored in DataFrames. Let's try out a few. Note that we call the method by using\n",
    "the object name *articles_df.method*. So `articles_df.columns` provides an index\n",
    "of all of the column names in our DataFrame.\n",
    "\n",
    "> ## Try out the methods below to see what they return.\n",
    ">\n",
    "> 1. `articles_df.columns`\n",
    "> 2. `articles_df.head()` - Also, what does `articles_df.head(15)` do?\n",
    "> 3. `articles_df.tail()`\n",
    "> 4. `articles_df.shape` - Take note of the output of the shape method. What format does it return the shape of the DataFrame in?\n",
    "{: .challenge}\n",
    "\n",
    "HINT: [More on tuples, here](https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences).\n",
    "\n",
    "## Calculating statistics from data in a Pandas DataFrame\n",
    "\n",
    "We've read our data into Python. Next, let's perform some quick summary\n",
    "statistics to learn more about the data that we're working with.\n",
    "We can perform summary stats quickly using groups. But\n",
    "first we need to figure out what we want to group by.\n",
    "\n",
    "Let's begin by exploring our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the column names\n",
    "articles_df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .source}\n",
    "\n",
    "which **returns**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array(['id', 'Title', 'Authors', 'DOI', 'URL', 'Subjects', 'ISSNs',\n",
    "       'Citation', 'LanguageId', 'LicenceId', 'Author_Count',\n",
    "       'First_Author', 'Citation_Count', 'Day', 'Month', 'Year'], dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .output}\n",
    "\n",
    "Let's get a list of all the months that articles were published in.\n",
    "The `pd.unique` function tells us all of the unique values in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(articles_df['Month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .source}\n",
    "\n",
    "which returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array([11, 12,  8,  4, 10,  9,  7,  6,  5,  3,  2,  1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .source}\n",
    "Which show us that articles have been published in every month of the year.\n",
    "\n",
    "> ## Challenge\n",
    ">\n",
    "> Create a list of unique ISSNs found in the articles data.\n",
    "> Call it `publications` (Note: each publication has a unique ISSN).\n",
    "> How many unique publications (ISSNs) are there in the data?\n",
    "{: .challenge}\n",
    "\n",
    "# Groups in Pandas\n",
    "Our DataFrame has a mixture of String and Numeric types. Some of the grouping\n",
    "operations work different for numeric types (e.g. calculating averages).\n",
    "\n",
    "We often want to calculate summary statistics grouped by subsets or attributes\n",
    "within fields of our data. For example, we might want to know the number of\n",
    "articles published in each publication.\n",
    "\n",
    "We can calculate basic statistics for all records in a single column using the\n",
    "syntax below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df['Citation_Count'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .source}\n",
    "gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count    1001.000000\n",
    "mean        9.023976\n",
    "std         1.655121\n",
    "min         3.000000\n",
    "25%         9.000000\n",
    "50%        10.000000\n",
    "75%        10.000000\n",
    "max        10.000000\n",
    "Name: Citation_Count, dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .output}\n",
    "\n",
    "We can also extract one specific metric if we wish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df['ISSNs'].unique()\n",
    "articles_df['ISSNs'].count()\n",
    "\n",
    "articles_df['Citation_Count'].min()\n",
    "articles_df['Citation_Count'].max()\n",
    "articles_df['Citation_Count'].mean()\n",
    "articles_df['Citation_Count'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .source}\n",
    "\n",
    "But if we want to summarize by one or more variables, for example Language, we can\n",
    "use Pandas' `.groupby` method. Once we've created a groupby DataFrame, we\n",
    "can quickly calculate summary statistics by a group of our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by Language\n",
    "byLang = articles_df.groupby('LanguageId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .source}\n",
    "\n",
    "The Pandas function `describe` will return descriptive stats including: mean,\n",
    "median, max, min, std and count for a particular column in the data. Pandas'\n",
    "`describe` function will only return summary values for columns containing\n",
    "numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics for all numeric columns by Language\n",
    "byLang.describe()\n",
    "# provide the mean for each numeric column by Language\n",
    "byLang.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .source}\n",
    "\n",
    "gives this output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    id  LicenceId  Author_Count  Citation_Count  Day  \\\n",
    "LanguageId                                                             \n",
    "1           510.004090   1.052147      4.013292        9.137014  1.0   \n",
    "2            52.533333   1.000000      3.333333        4.333333  1.0   \n",
    "3           116.571429   4.000000      4.142857        4.000000  1.0   \n",
    "4           112.000000   4.000000      5.000000        4.000000  1.0   \n",
    "\n",
    "                Month    Year  \n",
    "LanguageId                     \n",
    "1            6.332311  2015.0  \n",
    "2           11.000000  2015.0  \n",
    "3           12.000000  2015.0  \n",
    "4           12.000000  2015.0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .output}\n",
    "\n",
    "The `groupby` command is powerful in that it allows us to quickly generate\n",
    "summary stats.\n",
    "\n",
    "> ## Challenge\n",
    ">\n",
    "> 1. How many articles are published in each publication?\n",
    "> 2. What happens when you group by two columns using the following syntax and\n",
    ">    then grab mean values:\n",
    ">\n",
    ">    ~~~\n",
    ">    byMultiple = articles_df.groupby(['LanguageId', 'ISSNs'])\n",
    ">    byMultiple.mean()\n",
    ">    ~~~\n",
    ">    {: .source}\n",
    ">\n",
    "> 3. Summarize author counts for each publication (ISSNs) in your data.\n",
    ">    HINT: you can use the\n",
    ">    following syntax to only create summary statistics for one column in your data\n",
    ">    `by_ISSNs['Author_Count'].describe()`\n",
    "{: .challenge}\n",
    "\n",
    "> ## Did you get it right?\n",
    "> The Output from question 3 of the previous challenge looks like this:\n",
    ">\n",
    "> ~~~\n",
    "> ISSNs                     \n",
    "> 0367-0449|1988-3250  count    11.000000\n",
    ">                      mean      4.000000\n",
    ">                      std       2.720294\n",
    ">                      min       2.000000\n",
    ">                      25%       2.000000\n",
    ">                      50%       3.000000\n",
    ">                      75%       5.000000\n",
    ">                      max       9.000000\n",
    "> 1099-4300            count     1.000000\n",
    "> ...\n",
    "> ~~~\n",
    "> {: .output}\n",
    "{: .solution}\n",
    "\n",
    "## Quickly creating summary counts in Pandas\n",
    "\n",
    "Let's next count the number of articles for each publisher. We can do this in a few\n",
    "ways, but we'll use `groupby` combined with a `count()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of samples by publisher\n",
    "article_counts = articles_df.groupby('ISSNs')['Title'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .source}\n",
    "\n",
    "Or, we can also count just the rows that have the ISSN \"1420-3049\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df.groupby('ISSNs')['Title'].count()['1420-3049']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .source}\n",
    "\n",
    "## Basic Math functions\n",
    "\n",
    "If we wanted to, we could perform math on an entire column of our data. For\n",
    "example let's multiply all author count values by 2. A more practical use of this might\n",
    "be to normalize the data according to a mean, area, or some other value\n",
    "calculated from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply all author count values by 2\n",
    "articles_df['Author_Count'] * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .source}\n",
    "\n",
    "\n",
    "> ## Another Challenge\n",
    ">\n",
    "> 1. What's another way to create a list of licenses and associated `count` of the\n",
    ">    records in the data? Hint: you can perform `count`, `min`, etc functions on\n",
    ">    groupby DataFrames in the same way you can perform them on regular\n",
    ">    DataFrames.\n",
    "{:.challenge}\n",
    "\n",
    "\n",
    "# Quick & easy plotting data using Pandas\n",
    "\n",
    "We can plot our summary stats using Pandas, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure figures appear inline in Ipython Notebook\n",
    "%matplotlib inline\n",
    "# create a quick bar chart\n",
    "article_counts.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .source}\n",
    "\n",
    "#![Articles by ISSN Plot]({{ page.root }}/fig/articlesByISSN.png)\n",
    "Articles by ISSN plot\n",
    "\n",
    "We can also look at how many articles were published in each language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_count = articles_df.groupby('LanguageId')['Title'].count()\n",
    "# let's plot that too\n",
    "language_count.plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{: .source}\n",
    "\n",
    "> ## Activities\n",
    "> 1. Create a plot of average number of authors across all publishers.\n",
    "> 1. Create a plot of average number of authors across all publishers per language.\n",
    "{:.challenge}\n",
    "\n",
    "\n",
    "> ## Summary Plotting Challenge\n",
    ">\n",
    "> Create a stacked bar plot, with Number of articles on the Y axis, and the\n",
    "> stacked variable being `LicenceId`. The plot should show total number of articles\n",
    "> by license for each month. Some tips are below to help you solve this\n",
    "> challenge:\n",
    ">\n",
    "> * [For more on Pandas plots, visit this link.](http://pandas.pydata.org/pandas-docs/dev/generated/pandas.core.groupby.DataFrameGroupBy.plot.html)\n",
    "> * You can use the code that follows to create a stacked bar plot but the data to stack\n",
    ">   need to be in individual columns.  Here's a simple example with some data where\n",
    ">   'a', 'b', and 'c' are the groups, and 'one' and 'two' are the subgroups.\n",
    ">\n",
    "> ~~~\n",
    "> d = {'one' : pd.Series([1., 2., 3.], index=['a', 'b', 'c']),'two' : pd.Series([1., 2., 3., 4.], index=['a', 'b', 'c', 'd'])}\n",
    "> pd.DataFrame(d)\n",
    "> ~~~\n",
    "> {: .source}\n",
    ">\n",
    "> shows the following data\n",
    ">\n",
    "> ~~~\n",
    ">        one  two\n",
    ">    a    1    1\n",
    ">    b    2    2\n",
    ">    c    3    3\n",
    ">    d  NaN    4\n",
    "> ~~~\n",
    "> {: .output}\n",
    ">\n",
    "> We can plot the above with\n",
    ">\n",
    "> ~~~\n",
    "> # plot stacked data so columns 'one' and 'two' are stacked\n",
    "> my_df = pd.DataFrame(d)\n",
    "> my_df.plot(kind='bar',stacked=True,title=\"The title of my graph\")\n",
    "> ~~~\n",
    "> {: .source}\n",
    ">\n",
    "># ![Stacked Bar Plot]({{ page.root }}/fig//stackedBar1.png)\n",
    ">\n",
    "> * You can use the `.unstack()` method to transform grouped data into columns\n",
    "> for each plotting.  Try running `.unstack()` on some DataFrames above and see\n",
    "> what it yields.\n",
    ">\n",
    "> Start by transforming the grouped data (by `LicenceId` and `Month`) into an\n",
    "> unstacked layout, then create a stacked plot.\n",
    ">\n",
    "{:.challenge}\n",
    "\n",
    "> ## Solution to Summary Challenge\n",
    ">\n",
    "> First we group data by `Month` and by `LicenceId`, and then calculate a total for\n",
    "> each `Month`.\n",
    ">\n",
    "> ~~~\n",
    "> by_month_lic = articles_df.groupby(['Month','LicenceId'])\n",
    "> month_lic_count = by_month_lic.size()\n",
    "> ~~~\n",
    "> {: .source}\n",
    ">\n",
    "> This calculates the number of articles for each `Month` and `LicenceId` as a table\n",
    ">\n",
    "> ~~~\n",
    "> Month  LicenceId\n",
    "> 1      1             50\n",
    "> 2      1             96\n",
    "> 3      1             79\n",
    "> 4      1             69\n",
    ">        2              7\n",
    "> 5      1            107\n",
    "> 6      1             91\n",
    "> 7      1             94\n",
    "> 8      1             90\n",
    ">        2             13\n",
    "> 9      1             77\n",
    "> 10     1            104\n",
    "> 11     1             97\n",
    ">        2             10\n",
    ">        3              6\n",
    "> 12     4             11\n",
    "> dtype: int64\n",
    "> ~~~\n",
    "> {: .output}\n",
    ">\n",
    "> Below we'll use `.unstack()` on our grouped data to figure out the total weight\n",
    "> that each language contributed to each publisher.\n",
    ">\n",
    "> ~~~\n",
    "> by_month_lic = articles_df.groupby(['Month','LicenceId'])\n",
    "> month_lic_count = by_month_lic.size()\n",
    "> mlc = month_lic_count.unstack()\n",
    "> ~~~\n",
    "> {: .source}\n",
    ">\n",
    "> The `unstack` function above will display the following output:\n",
    ">\n",
    "> ~~~\n",
    "> LicenceId   1    2    3    4\n",
    "> Month\n",
    "> 1          50  NaN  NaN  NaN\n",
    "> 2          96  NaN  NaN  NaN\n",
    "> 3          79  NaN  NaN  NaN\n",
    "> 4          69    7  NaN  NaN\n",
    "> 5         107  NaN  NaN  NaN\n",
    "> 6          91  NaN  NaN  NaN\n",
    "> 7          94  NaN  NaN  NaN\n",
    "> 8          90   13  NaN  NaN\n",
    "> 9          77  NaN  NaN  NaN\n",
    "> 10        104  NaN  NaN  NaN\n",
    "> 11         97   10    6  NaN\n",
    "> 12        NaN  NaN  NaN   11\n",
    "> ~~~\n",
    "> {: .output}\n",
    ">\n",
    "> Now, create a stacked bar plot with that data where the article count for each\n",
    "> Month are stacked by License.\n",
    ">\n",
    "> Rather than display it as a table, we can plot the above data by stacking the\n",
    "> values of each licence as follows:\n",
    ">\n",
    "> ~~~\n",
    "> by_month_lic = articles_df.groupby(['Month','LicenceId'])\n",
    "> month_lic_count = by_month_lic.size()\n",
    "> mlc = month_lic_count.unstack()\n",
    ">\n",
    "> s_plot = mlc.plot(kind='bar',stacked=True,title=\"Total number of articles by Month and Licence\")\n",
    "> s_plot.set_ylabel(\"Licence Count\")\n",
    "> s_plot.set_xlabel(\"Month\");\n",
    "> ~~~\n",
    "> {: .source}\n",
    ">\n",
    "> #![Stacked Bar Plot]({{ page.root }}/fig/stackedBar.png)\n",
    "{:.solution}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
